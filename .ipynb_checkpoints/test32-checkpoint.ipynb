{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run all the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"test32\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomData = spark.read.csv('myFile0-1.csv', header='true', inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomData.createOrReplaceTempView('stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM stats\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SQL QUERY to get to display the records which have 3 or more consecutive events and the amount of people more than 100(\n",
    "\"\"\"\n",
    "spark.sql(\"\"\" \n",
    "SELECT DISTINCT( event_name ),\n",
    "               Max(people_count_sum)\n",
    "FROM   (SELECT *\n",
    "        FROM   (SELECT *,\n",
    "                       Sum(people_count)\n",
    "                         OVER (\n",
    "                           partition BY event_name\n",
    "                           ORDER BY id) AS people_count_sum\n",
    "                FROM   (SELECT *,\n",
    "                               Rank()\n",
    "                                 OVER (\n",
    "                                   partition BY event_name\n",
    "                                   ORDER BY id) AS cont_occurance\n",
    "                        FROM   stats\n",
    "                        ORDER  BY id)\n",
    "                ORDER  BY id)\n",
    "        WHERE  cont_occurance >= 3\n",
    "               AND people_count_sum >= 100)\n",
    "GROUP  BY event_name  \n",
    "\"\"\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data.csv', header='true', inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('VermontVendor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "'''select * from VermontVendor limit 10'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType,StringType\n",
    "from pyspark.sql.functions import udf,col\n",
    "import datetime\n",
    "\n",
    "def int_to_string(x):\n",
    "    return str(x)\n",
    "\n",
    "def datetime_formatter(dateStr):\n",
    "    import datetime\n",
    "    return (str(datetime.datetime.strptime(dateStr, \"%m/%d/%y %H:%M\").strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")))\n",
    "    \n",
    "toStr = udf(lambda z: int_to_string(z), StringType())\n",
    "parseDatetime = udf(lambda z: datetime_formatter(z), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifieddf = df.filter(col(\"Person Id\").isNotNull()) \\\n",
    "               .filter(col(\"Floor Access DateTime\").isNotNull()) \\\n",
    "               .filter(col(\"Floor Level\").isNotNull()) \\\n",
    "               .filter(col(\"Building\").isNotNull()) \\\n",
    "               .select(col(\"Floor Level\").alias(\"floor_level\"),\n",
    "                       col(\"Building\").alias(\"building\"),\n",
    "                       toStr(\"Person Id\").alias(\"person_id\"),\n",
    "                       parseDatetime(\"Floor Access DateTime\").alias(\"datetime\")\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifieddf.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifieddf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifieddf.write.mode(\"overwrite\")\\\n",
    "    .format('json') \\\n",
    "    .save('output_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
